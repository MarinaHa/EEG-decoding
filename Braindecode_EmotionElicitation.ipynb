{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braindecode_EmotionElicitation\n",
    "##### Read the features X and the targets y in and apply ConvNets to decode for the desired condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import pickle\n",
    "import braindecode\n",
    "import torch\n",
    "\n",
    "from mne.io import concatenate_raws\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "import torch as th\n",
    "from braindecode.experiments.monitors import compute_preds_per_trial_for_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature dictionary X and target dictionary y (subjIDs are the keys)\n",
    "X = pickle.load(open(\"X.pkl\", \"rb\"))\n",
    "y = pickle.load(open('y.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code from \n",
    "# https://robintibor.github.io/braindecode/notebooks/Cropped_Decoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogRate = np.empty([len(X), len(X)-1])\n",
    "\n",
    "#for subj in X: \n",
    "#    {i:a[i] for i in a if i!=0}\n",
    "\n",
    "subj = 5\n",
    "\n",
    "X_subj = [X[i] for i in X if i!= subj]\n",
    "y_subj = [y[i] for i in y if i!= subj]\n",
    "\n",
    "X_train = list()\n",
    "y_train = list()\n",
    "\n",
    "X_test = list()\n",
    "y_test = list()\n",
    "\n",
    "for trial in X[subj]: \n",
    "    X_test.append(trial.astype(np.float32))\n",
    "    \n",
    "for trial in y[subj]: \n",
    "    y_test.append(np.int64(trial))\n",
    "\n",
    "#X_test = X[subj].astype(np.float32)\n",
    "#y_test = y[subj].astype(np.int64)\n",
    "\n",
    "for subj in X_subj: \n",
    "    for trial in subj: \n",
    "        X_train.append(trial.astype(np.float32))\n",
    "\n",
    "for subj in y_subj: \n",
    "    for trial in subj: \n",
    "        y_train.append(np.int64(trial))\n",
    "        \n",
    "\n",
    "# Convert data from volt to millivolt\n",
    "\n",
    "\n",
    "train_set = SignalAndTarget(X_train, y_train)\n",
    "test_set = SignalAndTarget(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 predictions per input/trial\n",
      "Epoch 0\n",
      "Train  Loss: 1.01823\n",
      "Train  Accuracy: 48.2%\n",
      "Test   Loss: 1.18439\n",
      "Test   Accuracy: 45.0%\n",
      "Epoch 1\n",
      "Train  Loss: 1.01744\n",
      "Train  Accuracy: 50.0%\n",
      "Test   Loss: 1.11074\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 2\n",
      "Train  Loss: 1.02608\n",
      "Train  Accuracy: 46.7%\n",
      "Test   Loss: 1.09642\n",
      "Test   Accuracy: 40.0%\n",
      "Epoch 3\n",
      "Train  Loss: 0.99536\n",
      "Train  Accuracy: 48.2%\n",
      "Test   Loss: 1.09626\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 4\n",
      "Train  Loss: 1.00958\n",
      "Train  Accuracy: 47.4%\n",
      "Test   Loss: 1.05590\n",
      "Test   Accuracy: 45.0%\n",
      "Epoch 5\n",
      "Train  Loss: 0.98714\n",
      "Train  Accuracy: 51.1%\n",
      "Test   Loss: 1.15248\n",
      "Test   Accuracy: 30.0%\n",
      "Epoch 6\n",
      "Train  Loss: 1.01425\n",
      "Train  Accuracy: 48.5%\n",
      "Test   Loss: 1.23763\n",
      "Test   Accuracy: 30.0%\n",
      "Epoch 7\n",
      "Train  Loss: 1.00469\n",
      "Train  Accuracy: 47.8%\n",
      "Test   Loss: 1.08632\n",
      "Test   Accuracy: 45.0%\n",
      "Epoch 8\n",
      "Train  Loss: 1.03330\n",
      "Train  Accuracy: 44.2%\n",
      "Test   Loss: 1.03785\n",
      "Test   Accuracy: 50.0%\n",
      "Epoch 9\n",
      "Train  Loss: 0.99674\n",
      "Train  Accuracy: 48.2%\n",
      "Test   Loss: 1.17183\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 10\n",
      "Train  Loss: 0.99210\n",
      "Train  Accuracy: 48.9%\n",
      "Test   Loss: 1.10810\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 11\n",
      "Train  Loss: 0.98360\n",
      "Train  Accuracy: 48.9%\n",
      "Test   Loss: 1.05858\n",
      "Test   Accuracy: 40.0%\n",
      "Epoch 12\n",
      "Train  Loss: 0.98311\n",
      "Train  Accuracy: 52.9%\n",
      "Test   Loss: 1.58328\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 13\n",
      "Train  Loss: 0.97771\n",
      "Train  Accuracy: 50.4%\n",
      "Test   Loss: 1.00085\n",
      "Test   Accuracy: 50.0%\n",
      "Epoch 14\n",
      "Train  Loss: 0.97653\n",
      "Train  Accuracy: 50.7%\n",
      "Test   Loss: 1.16023\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 15\n",
      "Train  Loss: 0.96601\n",
      "Train  Accuracy: 50.7%\n",
      "Test   Loss: 1.07041\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 16\n",
      "Train  Loss: 0.96815\n",
      "Train  Accuracy: 50.0%\n",
      "Test   Loss: 1.19899\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 17\n",
      "Train  Loss: 0.96213\n",
      "Train  Accuracy: 52.2%\n",
      "Test   Loss: 1.08690\n",
      "Test   Accuracy: 25.0%\n",
      "Epoch 18\n",
      "Train  Loss: 0.96765\n",
      "Train  Accuracy: 49.6%\n",
      "Test   Loss: 1.07813\n",
      "Test   Accuracy: 35.0%\n",
      "Epoch 19\n",
      "Train  Loss: 0.96466\n",
      "Train  Accuracy: 50.7%\n",
      "Test   Loss: 1.19803\n",
      "Test   Accuracy: 35.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = torch.cuda.is_available()\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "input_time_length = 450\n",
    "n_classes = 3\n",
    "n_chans = 33 #train_set.X.shape[1]\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length=12).create_network()\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# determine output size\n",
    "test_input = np_to_var(np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "\n",
    "iterator = CropsFromTrialsIterator(batch_size=32,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "rng = RandomState((2017,6,30))\n",
    "for i_epoch in range(20):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(net_in)\n",
    "        # Mean predictions across trial\n",
    "        # Note that this will give identical gradients to computing\n",
    "        # a per-prediction loss (at least for the combination of log softmax activation\n",
    "        # and negative log likelihood loss which we are using here)\n",
    "        outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set),('Test', test_set)):\n",
    "        # Collect all predictions and losses\n",
    "        all_preds = []\n",
    "        all_losses = []\n",
    "        batch_sizes = []\n",
    "        for batch_X, batch_y in iterator.get_batches(dataset, shuffle=False):\n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            net_target = np_to_var(batch_y)\n",
    "            if cuda:\n",
    "                net_target = net_target.cuda()\n",
    "            outputs = model(net_in)\n",
    "            all_preds.append(var_to_np(outputs))\n",
    "            outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "            loss = F.nll_loss(outputs, net_target)\n",
    "            loss = float(var_to_np(loss))\n",
    "            all_losses.append(loss)\n",
    "            batch_sizes.append(len(batch_X))\n",
    "        # Compute mean per-input loss\n",
    "        loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "                       np.mean(batch_sizes))\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(setname, loss))\n",
    "        # Assign the predictions to the trials\n",
    "        preds_per_trial = compute_preds_per_trial_for_set(all_preds,\n",
    "                                                          input_time_length,\n",
    "                                                          dataset)\n",
    "        # preds per trial are now trials x classes x timesteps/predictions\n",
    "        # Now mean across timesteps for each trial to get per-trial predictions\n",
    "        meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "        predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "        accuracy = np.mean(predicted_labels == dataset.y)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
